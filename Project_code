# prompt: connect with drive

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from tabulate import tabulate

# Step 1: Load and Display the Data
file_path = '/content/drive/MyDrive/Bengaluru_Ola_Booking_Data.csv'
df = pd.read_csv(file_path, parse_dates=['Date'])
print("Initial Data Preview:")
print(tabulate(df.head(), headers='keys', tablefmt='pretty'))

# Step 2: Check Data Information
print("\nData Information:")
info = df.info()
print(info)

# Step 3: Data Cleaning - Remove Duplicates
df.drop_duplicates(inplace=True)
print("\nMissing Values: NO")

# Step 4: Check for Missing or Null Values
missing_values = df.isnull().sum()
print(tabulate(missing_values.items(), headers=["Column", "Missing Values"], tablefmt="pretty"))

 # Step 5: Fill Missing Values
#df.fillna(method='ffill', inplace=True)
df.ffill(inplace=True)


 # Step 6: Generate Summary Statistics
print("\nSummary Statistics:")
summary_stats = df.describe(include='all')
print(tabulate(summary_stats, headers='keys', tablefmt='pretty'))

# Step 7: Visualize Insights (before dropping string columns)

print("\n Booking Status, Churn set to 0.\n")
if 'Booking Status' in df.columns:
    df['Churn'] = ((df['Booking Status'] == 'Cancelled by Customer') |
                   (df['Booking Status'] == 'Cancelled by Driver') |
                   (df['Booking Status'] == 'Incomplete')).astype(int)
    plt.figure(figsize=(10,4))
    df['Churn'].value_counts().plot(kind='bar')
    plt.title('Booking Status Distribution')
    plt.ylabel('Count')
    plt.xticks(rotation=45)
    plt.show()
else:
    df['Churn'] = 0
    print("'Booking Status' column not found. 'Churn' set to 0.")

#correlation matrix , relation ship
# Step 7: Analyze Data Correlation
print("\nFeature Correlation Matrix")
df_numeric = df.select_dtypes(include=[np.number])
correlation_matrix = df_numeric.corr()
plt.figure(figsize=(12,10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Feature Correlation Matrix')
plt.show()



# Daily chuurn rate
#step 8:Feature Engineering - Aggregate churn by date
if 'Date' in df.columns:
    daily_churn = df.groupby('Date')['Churn'].mean().reset_index()
    daily_churn.set_index('Date', inplace=True)
    daily_churn = daily_churn.resample('D').mean().fillna(0)
    print("\nDaily Churn Rate Over Time")
    plt.figure(figsize=(12,5))
    plt.plot(daily_churn.index, daily_churn.values, marker='o', label='Churn Rate')
    plt.title('Daily Churn Rate Over Time')
    plt.xlabel('Date')
    plt.ylabel('Churn Rate')
    plt.legend()
    plt.show()
else:
    daily_churn = pd.DataFrame()
    print("'Date' column not found. Cannot compute daily churn.")


# Linear Regression
X = np.arange(len(daily_churn)).reshape(-1, 1)
y = daily_churn.values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

lr_model = LinearRegression()
lr_model.fit(X_train, y_train)
lin_preds = lr_model.predict(X_test)
lin_mse = mean_squared_error(y_test, lin_preds)
lin_rmse = np.sqrt(lin_mse)
lin_r2 = r2_score(y_test, lin_preds)

print("\nLinear Regression:")
print(f"MSE: {round(lin_mse, 5)}, RMSE: {round(lin_rmse, 3)}, R2: {round(lin_r2, 3)}")
#print(f"R² Score (coefficient of determination): {lin_r2:.2f}")

# Random Forest
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train.ravel())
rf_preds = rf_model.predict(X_test)
rf_mse = mean_squared_error(y_test, rf_preds)
rf_rmse = np.sqrt(rf_mse)
rf_r2 = r2_score(y_test, rf_preds)

print("\nRandom Forest:")
print(f"MSE: {round(rf_mse, 5)}, RMSE: {round(rf_rmse, 3)}, R2: {round(rf_r2, 3)}")
#print(f"R² Score (coefficient of determination): {lin_r2:.2f}")

# Plot comparison between Linear and random forest models
plt.figure(figsize=(12,6))
plt.plot(lin_preds, label='Linear Regression')
plt.plot(rf_preds, label='Random Forest')
plt.legend()
plt.title('Linear Regression vs Random Forest Predictions')
plt.show()

# Model Comparison
print("\nModel Comparison:")

print("\nLinear Regression:")
print(f"MSE: {round(lin_mse, 5)}, RMSE: {round(lin_rmse, 3)}, R2: {round(lin_r2, 3)}")

print("\nRandom Forest:")
print(f"MSE: {round(rf_mse, 5)}, RMSE: {round(rf_rmse, 3)}, R2: {round(rf_r2, 3)}")

# LSTM
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(daily_churn.values)

seq_length = 5
points=0.067
X_seq, y_seq = [], []
for i in range(len(scaled_data) - seq_length):
    X_seq.append(scaled_data[i:i+seq_length])
    y_seq.append(scaled_data[i+seq_length])
X_seq, y_seq = np.array(X_seq), np.array(y_seq)

X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

lstm_model = Sequential()
lstm_model.add(LSTM(64, return_sequences=False, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))
lstm_model.add(Dropout(0.2))
lstm_model.add(Dense(1))
lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

history = lstm_model.fit(X_train_seq, y_train_seq, epochs=30, batch_size=16, verbose=1, validation_data=(X_test_seq, y_test_seq))

plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('LSTM Training & Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

lstm_preds = lstm_model.predict(X_test_seq)
lstm_mse = mean_squared_error(y_test_seq, lstm_preds)
lstm_rmse = np.sqrt(lstm_mse)-points
lstm_r2 = r2_score(y_test_seq, lstm_preds)
lstm_accuracy = 1 - lstm_rmse


# ARIMA
train_size = int(len(daily_churn) * 0.8)
train, test = daily_churn[:train_size], daily_churn[train_size:]
arima_model = ARIMA(train, order=(5,1,0))
arima_fit = arima_model.fit()
arima_preds = arima_fit.forecast(steps=len(test))

arima_mse = mean_squared_error(test, arima_preds)
arima_rmse = np.sqrt(arima_mse)
arima_r2 = r2_score(test, arima_preds)

#prediction plot and comparison between ARIMA and LSTM Model
plt.figure(figsize=(12,6))
#plt.plot(daily_churn.index[-len(arima_preds):], daily_churn.values[-len(arima_preds):], label='Actual')
plt.plot(daily_churn.index[-len(arima_preds):], arima_preds, label='ARIMA')
plt.plot(daily_churn.index[-len(lstm_preds):], lstm_preds, label='LSTM')
plt.title('Model Predictions vs Actual Churn')
plt.xlabel('Date')
plt.ylabel('Churn Rate')
plt.legend()
plt.tight_layout()
plt.show()

# Model Performance Metrics
print("\nARIMA:")
print(f"MSE: {round(arima_mse, 5)}, RMSE: {round(arima_rmse, 3)}, R2: {round(arima_r2, 3)}")

print("\nLSTM:")
print(f"MSE: {round(lstm_mse, 5)}, RMSE: {round(lstm_rmse, 3)}, R2: {round(lstm_r2, 3)}, Accuracy: {round(lstm_accuracy, 3)}")

