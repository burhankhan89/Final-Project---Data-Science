import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_recall_fscore_support, confusion_matrix
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingClassifier
from sklearn.impute import SimpleImputer
from statsmodels.tsa.arima.model import ARIMA
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from sklearn.model_selection import train_test_split, TimeSeriesSplit, GridSearchCV
from tabulate import tabulate
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')


# Step 1: Load and Display the Data
file_path = '/content/drive/MyDrive/Bengaluru_Ola_Booking_Data.csv'
df = pd.read_csv(file_path, parse_dates=['Date'])
print("Initial Data Preview:")
print(tabulate(df.head(), headers='keys', tablefmt='pretty'))


# Step 2: Check Data Information
print("\nData Information:")
info = df.info()
print(info)


# Step 3: Enhanced Data Cleaning

df.drop_duplicates(inplace=True)
print(f"\nDuplicates removed. New shape: {df.shape}")


# Check for missing values
missing_values = df.isnull().sum()
print("\nMissing Values:")
print(tabulate(missing_values.items(), headers=["Column", "Missing Values"], tablefmt="pretty"))


# Fill missing values with more appropriate methods based on column type
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:
        df[col].fillna(df[col].median(), inplace=True)

print("\nChecking for remaining NaN values...")
print(df.isnull().sum().sum(), "total NaN values remaining")



# Step 4: Feature Engineering
print("\nPerforming Feature Engineering...")

df['DayOfWeek'] = df['Date'].dt.dayofweek
df['Weekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 5 else 0)
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day
df['Hour'] = pd.to_datetime(df['Pickup Time']).dt.hour if 'Pickup Time' in df.columns else 0

if 'Booking Status' in df.columns:

    df['Churn'] = ((df['Booking Status'] == 'Cancelled by Customer') |
                   (df['Booking Status'] == 'Cancelled by Driver') |
                   (df['Booking Status'] == 'Incomplete')).astype(int)

    plt.figure(figsize=(10,5))
    sns.countplot(x='Churn', data=df)
    plt.title('Churn Distribution')
    plt.ylabel('Count')
    plt.xticks([0, 1], ['Retained', 'Churned'])
    plt.show()

    status_mapping = {
        'Completed': 0,
        'Cancelled by Customer': 1,
        'Cancelled by Driver': 2,
        'Incomplete': 3
    }

    df['BookingStatusCategory'] = df['Booking Status'].map(
        lambda x: status_mapping.get(x, 4) if pd.notnull(x) else 4
    )
else:
    df['Churn'] = 0
    print("'Booking Status' column not found. 'Churn' set to 0.")

if 'Customer ID' in df.columns:

    customer_metrics = df.groupby('Customer ID').agg({
        'Churn': ['mean', 'sum', 'count'],
        'Date': ['min', 'max']
    })

    customer_metrics.columns = ['ChurnRate', 'TotalChurns', 'TotalBookings', 'FirstBooking', 'LastBooking']
    customer_metrics['CustomerLifetime'] = (customer_metrics['LastBooking'] - customer_metrics['FirstBooking']).dt.days
    customer_metrics['BookingFrequency'] = customer_metrics['TotalBookings'] / np.maximum(customer_metrics['CustomerLifetime'], 1)

    df = df.merge(customer_metrics, left_on='Customer ID', right_index=True, how='left')



# Step 5: Exploratory Data Analysis
print("\nPerforming Exploratory Data Analysis...")

plt.figure(figsize=(12,5))
churn_by_day = df.groupby('DayOfWeek')['Churn'].mean()
sns.barplot(x=churn_by_day.index, y=churn_by_day.values)
plt.title('Churn Rate by Day of Week')
plt.xlabel('Day of Week (0=Monday, 6=Sunday)')
plt.ylabel('Churn Rate')
plt.show()

if 'Pickup Lat' in df.columns and 'Pickup Long' in df.columns:
    plt.figure(figsize=(12,10))
    plt.scatter(df['Pickup Long'], df['Pickup Lat'], c=df['Churn'], cmap='coolwarm', alpha=0.6)
    plt.title('Churn by Pickup Location')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    plt.colorbar(label='Churn')
    plt.show()

df_numeric = df.select_dtypes(include=[np.number])
correlation_matrix = df_numeric.corr()
plt.figure(figsize=(14,12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Feature Correlation Matrix')
plt.tight_layout()
plt.show()



# Step 6: Time Series Analysis - Daily churn rate
if 'Date' in df.columns:
    daily_churn = df.groupby('Date')['Churn'].mean().reset_index()
    daily_churn.set_index('Date', inplace=True)
    daily_churn = daily_churn.resample('D').mean().fillna(method='ffill')

    daily_churn['day_of_week'] = daily_churn.index.dayofweek
    daily_churn['month'] = daily_churn.index.month
    daily_churn['trend'] = np.arange(len(daily_churn))

    plt.figure(figsize=(14,6))
    plt.plot(daily_churn.index, daily_churn['Churn'], marker='o', linestyle='-', markersize=4)
    plt.title('Daily Churn Rate Over Time')
    plt.xlabel('Date')
    plt.ylabel('Churn Rate')
    plt.grid(True, alpha=0.3)

    for window in [7, 14, 30]:
        daily_churn[f'MA_{window}'] = daily_churn['Churn'].rolling(window=window).mean()
        plt.plot(daily_churn.index, daily_churn[f'MA_{window}'],
                 label=f'{window}-day Moving Average')

    plt.legend()
    plt.show()
else:
    daily_churn = pd.DataFrame()
    print("'Date' column not found. Cannot compute daily churn.")
